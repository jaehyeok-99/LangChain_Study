from langchain_ollama import ChatOllama
from langchain_text_splitters import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough
import gradio as gr

# ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
llm = ChatOllama(model="llama3.1:8b")
hf_embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

prompt_template = ChatPromptTemplate.from_template("""
<context>{context}</context>
<question>{input}</question>
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì…ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥ì„ í† ëŒ€ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ë§Œì•½, ë¬¸ë§¥ì—ì„œ ë‹µë³€ì„ ìœ„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 'ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' ë¼ê³  ë‹µí•˜ì„¸ìš”.
ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤ë©´ í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
""")

# ì „ì—­ ìƒíƒœ ë³€ìˆ˜
db, retriever, rag_chain = None, None, None

def load_pdf(file):
    global db, retriever, rag_chain
    try:
        loader = PyPDFLoader(file.name)
        docs = loader.load_and_split(text_splitter)
        db = Chroma.from_documents(docs, hf_embeddings, collection_name="temp_collection")
        retriever = db.as_retriever(search_kwargs={"k": 3})
        rag_chain = (
            {"context": retriever, "input": RunnablePassthrough()} 
            | prompt_template 
            | llm 
            | StrOutputParser()
        )
        return "âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."
    except Exception as e:
        return f"âŒ PDF íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"

def respond(message, chat_history):
    if not rag_chain:
        return "ğŸ“Œ ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”", chat_history
    try:
        response = rag_chain.invoke(message)
        chat_history.append((message, response))
        return "", chat_history
    except Exception as e:
        return f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}", chat_history

def clear_all():
    global db, retriever, rag_chain
    if db is not None:
        try:
            db._client.delete_collection(db._collection.name)
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì˜¤ë¥˜: {e}")
    db, retriever, rag_chain = None, None, None
    return None, "", []

with gr.Blocks(theme=gr.themes.Ocean()) as demo:
    gr.Markdown("""
    # âœ¨RAG PDF ê¸°ë°˜ AI ì±—ë´‡ (ë¡œì»¬)
    **PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µ**
    ###### ğŸŒì›¹ ì¸í„°í˜ì´ìŠ¤ : Gradio          
    ###### ğŸ’¬LLM ëª¨ë¸ : llama3.1:8b
    ###### ğŸ”—ì„ë² ë”© ëª¨ë¸ : BAAI/bge-m3
    ###### ğŸ“ˆë²¡í„°DB : Chroma
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            file_input = gr.File(label="ğŸ“„ PDF ì—…ë¡œë“œ", file_types=[".pdf"])
            with gr.Row():
                upload_btn = gr.Button("ğŸ“¤ ì—…ë¡œë“œ", variant="primary")
                clear_btn = gr.Button("ğŸ”„ ì´ˆê¸°í™”", variant="secondary")
            status = gr.Textbox(label="ğŸ”” ì‹œìŠ¤í…œ ìƒíƒœ", interactive=False)
        
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(height=500)
            msg = gr.Textbox(label="ğŸ’¬ ë©”ì‹œì§€ ì…ë ¥", placeholder="PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...")
            submit_btn = gr.Button("ğŸ“¨ ì „ì†¡", variant="primary")

    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    upload_btn.click(load_pdf, inputs=file_input, outputs=status)
    clear_btn.click(clear_all, outputs=[file_input, status, chatbot])
    msg.submit(respond, [msg, chatbot], [msg, chatbot])
    submit_btn.click(respond, [msg, chatbot], [msg, chatbot])

demo.launch(inbrowser=True)