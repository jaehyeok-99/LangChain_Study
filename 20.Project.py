# ì¸ê³µì§€ëŠ¥ PDF Q&A ì±—ë´‡ í”„ë¡œì íŠ¸

#from dotenv import load_dotenv #api í‚¤ .envìœ¼ë¡œ ê´€ë¦¬í• ê²½ìš°
#load_dotenv()

import gradio as gr
#from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_text_splitters import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough

# í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
#load_dotenv()

# LLM ì„¤ì •(ìœ ë£Œ,ë¬´ë£Œ)
#llm = ChatOpenAI(model="gpt-4o-mini", api_key="ì•„ì´í”¼ í‚¤ ì…ë ¥") 
llm = ChatOllama(model="llama3.1:8b")

# í…ìŠ¤íŠ¸ ë¶„ë¦¬
text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)

# ì„ë² ë”© ëª¨ë¸
hf_embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
message = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì…ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥ì„ í† ëŒ€ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ë§Œì•½, ë¬¸ë§¥ì—ì„œ ë‹µë³€ì„ ìœ„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 'ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' ë¼ê³  ë‹µí•˜ì„¸ìš”.
ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤ë©´ í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

## ì£¼ì–´ì§„ ë¬¸ë§¥:
{context}

## ì‚¬ìš©ì ì§ˆë¬¸:
{input}
"""
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("human", message)
    ]
)

# ì¶œë ¥ íŒŒì„œ
parser = StrOutputParser()

# ì „ì—­ ë³€ìˆ˜
db = None
retriever = None
rag_chain = None

def load_pdf(file):
    global db, retriever, rag_chain

    loader = PyPDFLoader(file.name)
    docs = loader.load_and_split(text_splitter=text_splitter)

    db = FAISS.from_documents(docs, hf_embeddings)
    retriever = db.as_retriever(search_kwargs={"k": 3})

    rag_chain = {
        "context": retriever,
        "input": RunnablePassthrough()
    } | prompt_template | llm | parser

    return "PDF íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."

def answer_question(question):
    if rag_chain is None:
        return "ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
    return rag_chain.invoke(question)

with gr.Blocks(theme=gr.themes.Ocean()) as demo:
    gr.Markdown("""
    # âœ¨RAG 
    **PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µ(ë¡œì»¬o)**
    ###### ğŸ’¬LLM ëª¨ë¸ : llama3.1:8b
    ###### ğŸ”—ì„ë² ë”© ëª¨ë¸ : BAAI/bge-m3
    ###### ğŸ“ˆë²¡í„°DB : Chroma
    
""")

    with gr.Row():
        with gr.Column(scale=1):
            file_input = gr.File(label="PDF íŒŒì¼ ì—…ë¡œë“œ")
            upload_button = gr.Button("ğŸ“¤ ì—…ë¡œë“œ ë° ì²˜ë¦¬")
            status_output = gr.Textbox(label="ğŸ“¢ ìƒíƒœ ë©”ì‹œì§€")

        with gr.Column(scale=2):
            question_input = gr.Textbox(label="ğŸ’¬ ì§ˆë¬¸ ì…ë ¥", placeholder="ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì ì–´ì£¼ì„¸ìš”.")
            submit_button = gr.Button("âœ…ë‹µë³€ ë°›ê¸°")
            answer_output = gr.Textbox(label="ğŸ“ AI ë‹µë³€")

    upload_button.click(load_pdf, inputs=file_input, outputs=status_output)
    submit_button.click(answer_question, inputs=question_input, outputs=answer_output)

demo.launch()